{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "851af380-15ef-438e-ad51-04472348f1e6",
   "metadata": {},
   "source": [
    "### Step 1. prepare Dataset + DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78c4fa93-f636-4f5a-ac6b-92effa9dda06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Twitter15Dataset(Dataset):\n",
    "    def __init__(self, graph_data_list):\n",
    "        \"\"\"\n",
    "        Custom dataset for Twitter15 graph data.\n",
    "\n",
    "        Args:\n",
    "            graph_data_list (list): List of graph samples. \n",
    "                                    Each graph is a dictionary with keys: \n",
    "                                    - 'x': node feature matrix (Tensor of shape [seq_len, feature_dim])\n",
    "                                    - 'edge_index': edge list (not used here)\n",
    "                                    - 'y': label (int in [0, 3])\n",
    "        \"\"\"\n",
    "        self.graphs = graph_data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of samples in the dataset\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the features and label for a given index\n",
    "        graph = self.graphs[idx]\n",
    "        x = graph['x']  # Feature tensor: (sequence_length, feature_dim)\n",
    "        y = graph['y']  # Label: integer in range [0, 3]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function to handle batches of variable-length sequences.\n",
    "    Pads each sequence in the batch to the length of the longest sequence.\n",
    "\n",
    "    Args:\n",
    "        batch (list of tuples): Each tuple is (x, y), where\n",
    "                                - x: Tensor of shape (seq_len, feature_dim)\n",
    "                                - y: label (int)\n",
    "\n",
    "    Returns:\n",
    "        padded_xs (Tensor): Padded input features (batch_size, max_seq_len, feature_dim)\n",
    "        masks (Tensor): Boolean masks indicating valid positions (batch_size, max_seq_len)\n",
    "        ys (Tensor): Tensor of labels (batch_size,)\n",
    "    \"\"\"\n",
    "    xs, ys = zip(*batch)\n",
    "\n",
    "    # Determine the maximum sequence length in this batch\n",
    "    max_len = max(x.shape[0] for x in xs)\n",
    "    feature_dim = xs[0].shape[1]\n",
    "\n",
    "    padded_xs = []\n",
    "    masks = []\n",
    "\n",
    "    for x in xs:\n",
    "        seq_len = x.shape[0]\n",
    "        pad_len = max_len - seq_len\n",
    "\n",
    "        if pad_len > 0:\n",
    "            # Pad with zeros at the end of the sequence\n",
    "            pad = torch.zeros((pad_len, feature_dim), dtype=x.dtype)\n",
    "            x_padded = torch.cat([x, pad], dim=0)\n",
    "        else:\n",
    "            x_padded = x\n",
    "\n",
    "        # Create a mask where True indicates valid (non-padded) positions\n",
    "        mask = torch.cat([torch.ones(seq_len), torch.zeros(pad_len)]).bool()\n",
    "\n",
    "        padded_xs.append(x_padded)\n",
    "        masks.append(mask)\n",
    "\n",
    "    # Stack all sequences and masks into tensors\n",
    "    padded_xs = torch.stack(padded_xs)    # Shape: (batch_size, max_len, feature_dim)\n",
    "    masks = torch.stack(masks)            # Shape: (batch_size, max_len)\n",
    "    ys = torch.tensor(ys)                 # Shape: (batch_size,)\n",
    "\n",
    "    return padded_xs, masks, ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d2a3f99-0e53-4ce0-b328-e5a32368170a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1043, Val: 223, Test: 224\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# Load your cleaned graph data\n",
    "graph_data_list = torch.load(\"../processed/twitter15_graph_data_clean.pt\", weights_only=False)\n",
    "\n",
    "# Split into Train / Validation / Test sets in a 7:1.5:1.5 ratio\n",
    "train_graphs, temp_graphs = train_test_split(graph_data_list, test_size=0.3, random_state=42)\n",
    "val_graphs, test_graphs = train_test_split(temp_graphs, test_size=0.5, random_state=42)\n",
    "\n",
    "# Output the number of samples in each split\n",
    "print(f\"Train: {len(train_graphs)}, Val: {len(val_graphs)}, Test: {len(test_graphs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1a066f7-510d-4732-bb5e-775da05f5cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created successfully!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Build Dataset objects for training, validation, and test sets\n",
    "train_dataset = Twitter15Dataset(train_graphs)\n",
    "val_dataset = Twitter15Dataset(val_graphs)\n",
    "test_dataset = Twitter15Dataset(test_graphs)\n",
    "\n",
    "# Build DataLoaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(\"DataLoaders created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820b6a26-4e2c-4b7e-9e8d-fe2966327260",
   "metadata": {},
   "source": [
    "### Step 2: Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b613c42f-d68d-417c-ac2a-95ae85b3675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, feature_dim, hidden_dim, num_classes, num_heads=8, num_layers=2, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer-based classifier model.\n",
    "\n",
    "        Args:\n",
    "            feature_dim (int): The dimension of input features.\n",
    "            hidden_dim (int): The dimension of the feedforward network within the transformer encoder.\n",
    "            num_classes (int): The number of output classes for classification.\n",
    "            num_heads (int, optional): The number of attention heads in the transformer encoder. Default is 8.\n",
    "            num_layers (int, optional): The number of layers in the transformer encoder. Default is 2.\n",
    "            dropout (float, optional): The dropout rate for regularization. Default is 0.1.\n",
    "        \"\"\"\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        \n",
    "        # Projection layer to map input features to a fixed size (768)\n",
    "        self.input_projection = nn.Linear(feature_dim, 768)  # Maps input feature dimension to 768\n",
    "        \n",
    "        # Create a transformer encoder layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=768,                 # Input dimension (should match the transformer model size)\n",
    "            nhead=num_heads,             # Number of attention heads\n",
    "            dim_feedforward=hidden_dim,  # Hidden layer size for the feedforward network\n",
    "            dropout=dropout,             # Dropout rate for regularization\n",
    "            batch_first=True             # The input tensor format will be (batch_size, seq_len, feature_dim)\n",
    "        )\n",
    "        \n",
    "        # Transformer encoder which consists of multiple encoder layers\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers  # Number of encoder layers\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(768, num_classes)  # Output layer mapping from transformer output to class scores\n",
    "        self.dropout = nn.Dropout(dropout)     # Dropout layer for regularization\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "\n",
    "        Args:\n",
    "            x (tensor): Input feature tensor of shape (batch_size, seq_len, feature_dim).\n",
    "            mask (tensor): Mask tensor of shape (batch_size, seq_len), where 1 indicates valid positions and 0 indicates padding.\n",
    "\n",
    "        Returns:\n",
    "            tuple: \n",
    "                - logits (tensor): The class scores, shape (batch_size, num_classes).\n",
    "                - transformer_out (tensor): The output from the transformer encoder, shape (batch_size, seq_len, 768).\n",
    "        \"\"\"\n",
    "        src_key_padding_mask = ~mask  # Create padding mask, inverted to match the expected format\n",
    "\n",
    "        # Project the input features to the transformer input size (768)\n",
    "        x = self.input_projection(x)  # Adds the projection layer to adjust feature dimensions\n",
    "\n",
    "        # Pass the input through the transformer encoder\n",
    "        transformer_out = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        # Pooling the transformer output by averaging over the sequence length\n",
    "        pooled_output = transformer_out.mean(dim=1)  # Shape: (batch_size, 768)\n",
    "\n",
    "        # Apply dropout for regularization\n",
    "        output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Final classification layer\n",
    "        logits = self.fc(output)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "        return logits, transformer_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6690684a-800d-4030-b1eb-d77aef088022",
   "metadata": {},
   "source": [
    "### Step 3. Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56735c89-d1ba-4314-8468-98f6df234351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, loss_fn, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        train_loader (DataLoader): DataLoader for the training dataset.\n",
    "        optimizer (Optimizer): Optimizer to update model parameters.\n",
    "        loss_fn (function): Loss function to compute the loss.\n",
    "        device (torch.device): Device to move the data and model (e.g., 'cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - epoch_loss (float): The average loss for the epoch.\n",
    "            - epoch_acc (float): The accuracy for the epoch.\n",
    "            - epoch_f1 (float): The F1 score for the epoch (macro average).\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for x, mask, y in train_loader:\n",
    "        x = x.to(device)  # Move data to the specified device\n",
    "        mask = mask.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Replace NaN values in x with 0\n",
    "        x = torch.nan_to_num(x, nan=0.0)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, _ = model(x, mask)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        loss.backward()  # Backpropagation\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients to avoid exploding gradients\n",
    "        optimizer.step()  # Update model parameters\n",
    "\n",
    "        running_loss += loss.item() * x.size(0)  # Accumulate loss\n",
    "\n",
    "        # Get predictions\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        all_preds.extend(preds.detach().cpu().tolist())  # Detach tensor and move to CPU for storing predictions\n",
    "        all_labels.extend(y.cpu().tolist())  # Store true labels\n",
    "\n",
    "    # Calculate metrics\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)  # Accuracy calculation\n",
    "    epoch_f1 = f1_score(all_labels, all_preds, average='macro')  # F1 score calculation (macro average)\n",
    "\n",
    "    return epoch_loss, epoch_acc, epoch_f1\n",
    "\n",
    "def evaluate_one_epoch(model, val_loader, loss_fn, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to evaluate.\n",
    "        val_loader (DataLoader): DataLoader for the validation dataset.\n",
    "        loss_fn (function): Loss function to compute the loss.\n",
    "        device (torch.device): Device to move the data and model (e.g., 'cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - epoch_loss (float): The average loss for the epoch.\n",
    "            - epoch_acc (float): The accuracy for the epoch.\n",
    "            - epoch_f1 (float): The F1 score for the epoch (macro average).\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for x, mask, y in val_loader:\n",
    "            x = x.to(device)  # Move data to the specified device\n",
    "            mask = mask.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Replace NaN values in x with 0\n",
    "            x = torch.nan_to_num(x, nan=0.0)\n",
    "\n",
    "            # Forward pass\n",
    "            logits, _ = model(x, mask)\n",
    "            logits = torch.clamp(logits, min=-10, max=10)  # Clamp logits to avoid large values\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(logits, y)\n",
    "\n",
    "            running_loss += loss.item() * x.size(0)  # Accumulate loss\n",
    "\n",
    "            # Get predictions\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            all_preds.extend(preds.cpu().tolist())  # Detach tensor and move to CPU for storing predictions\n",
    "            all_labels.extend(y.cpu().tolist())  # Store true labels\n",
    "\n",
    "    # Calculate metrics\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)  # Accuracy calculation\n",
    "    epoch_f1 = f1_score(all_labels, all_preds, average='macro')  # F1 score calculation (macro average)\n",
    "\n",
    "    return epoch_loss, epoch_acc, epoch_f1\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    EarlyStopping class to monitor validation performance and stop training early when the performance does not improve.\n",
    "\n",
    "    Args:\n",
    "        patience (int): The number of epochs to wait for improvement before stopping.\n",
    "        verbose (bool): Whether to print messages when early stopping is triggered.\n",
    "        delta (float): Minimum change to qualify as an improvement.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, verbose=False, delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.counter = 0  # Counter to track the number of epochs without improvement\n",
    "        self.best_score = None  # Best validation score observed so far\n",
    "        self.early_stop = False  # Flag to indicate early stopping\n",
    "        self.best_f1 = -float('inf')  # Best F1 score observed so far\n",
    "\n",
    "    def __call__(self, val_f1, model, save_path):\n",
    "        \"\"\"\n",
    "        Checks if early stopping should be triggered based on validation F1 score.\n",
    "\n",
    "        Args:\n",
    "            val_f1 (float): The current validation F1 score.\n",
    "            model (nn.Module): The model being trained.\n",
    "            save_path (str): The path to save the model checkpoint.\n",
    "        \"\"\"\n",
    "        score = val_f1\n",
    "\n",
    "        # Check if this is the first epoch or if the score has improved\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_f1, model, save_path)\n",
    "        elif score < self.best_score + self.delta:  # If no improvement\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:  # If patience is exceeded, stop early\n",
    "                self.early_stop = True\n",
    "        else:  # If the score improved, reset the counter and update the best score\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_f1, model, save_path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_f1, model, save_path):\n",
    "        \"\"\"\n",
    "        Saves the model checkpoint if validation F1 score improves.\n",
    "\n",
    "        Args:\n",
    "            val_f1 (float): The current validation F1 score.\n",
    "            model (nn.Module): The model being trained.\n",
    "            save_path (str): The path to save the model checkpoint.\n",
    "        \"\"\"\n",
    "        torch.save(model.state_dict(), save_path)  # Save the model state dict\n",
    "        self.best_f1 = val_f1  # Update the best F1 score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86a168a-6e41-461a-aa73-4a7014c0ec9c",
   "metadata": {},
   "source": [
    "### Step 4: Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ae78cff-6175-45b9-8539-2b5fb3d5ec35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/xzhan/.conda/envs/env_214/lib/python3.13/site-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "  Train Loss: 1.3873 | Train Acc: 0.3557 | Train F1: 0.3545\n",
      "  Val   Loss: 1.3492 | Val   Acc: 0.2960 | Val   F1: 0.1640\n",
      "Epoch 2:\n",
      "  Train Loss: 1.1785 | Train Acc: 0.5014 | Train F1: 0.4958\n",
      "  Val   Loss: 1.3182 | Val   Acc: 0.3004 | Val   F1: 0.1620\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 3:\n",
      "  Train Loss: 1.0932 | Train Acc: 0.5781 | Train F1: 0.5721\n",
      "  Val   Loss: 1.2857 | Val   Acc: 0.4933 | Val   F1: 0.4708\n",
      "Epoch 4:\n",
      "  Train Loss: 0.9971 | Train Acc: 0.6520 | Train F1: 0.6484\n",
      "  Val   Loss: 1.2808 | Val   Acc: 0.4170 | Val   F1: 0.3521\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 5:\n",
      "  Train Loss: 0.9067 | Train Acc: 0.6855 | Train F1: 0.6833\n",
      "  Val   Loss: 1.3070 | Val   Acc: 0.3363 | Val   F1: 0.2311\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 6:\n",
      "  Train Loss: 0.8266 | Train Acc: 0.7555 | Train F1: 0.7541\n",
      "  Val   Loss: 1.2557 | Val   Acc: 0.4126 | Val   F1: 0.3503\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 7:\n",
      "  Train Loss: 0.7364 | Train Acc: 0.8102 | Train F1: 0.8087\n",
      "  Val   Loss: 1.2391 | Val   Acc: 0.4529 | Val   F1: 0.4114\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 8:\n",
      "  Train Loss: 0.6534 | Train Acc: 0.8600 | Train F1: 0.8584\n",
      "  Val   Loss: 1.2377 | Val   Acc: 0.4215 | Val   F1: 0.3733\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 9:\n",
      "  Train Loss: 0.5630 | Train Acc: 0.9195 | Train F1: 0.9189\n",
      "  Val   Loss: 1.2261 | Val   Acc: 0.4529 | Val   F1: 0.3988\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch 10:\n",
      "  Train Loss: 0.4998 | Train Acc: 0.9559 | Train F1: 0.9557\n",
      "  Val   Loss: 1.2411 | Val   Acc: 0.5112 | Val   F1: 0.4736\n",
      "Epoch 11:\n",
      "  Train Loss: 0.4690 | Train Acc: 0.9799 | Train F1: 0.9797\n",
      "  Val   Loss: 1.2301 | Val   Acc: 0.4843 | Val   F1: 0.4388\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 12:\n",
      "  Train Loss: 0.4393 | Train Acc: 0.9875 | Train F1: 0.9876\n",
      "  Val   Loss: 1.2016 | Val   Acc: 0.4978 | Val   F1: 0.4635\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 13:\n",
      "  Train Loss: 0.4125 | Train Acc: 0.9971 | Train F1: 0.9971\n",
      "  Val   Loss: 1.2319 | Val   Acc: 0.4529 | Val   F1: 0.4190\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 14:\n",
      "  Train Loss: 0.4021 | Train Acc: 0.9971 | Train F1: 0.9971\n",
      "  Val   Loss: 1.2188 | Val   Acc: 0.5112 | Val   F1: 0.5017\n",
      "Epoch 15:\n",
      "  Train Loss: 0.3973 | Train Acc: 0.9990 | Train F1: 0.9990\n",
      "  Val   Loss: 1.2254 | Val   Acc: 0.4843 | Val   F1: 0.4631\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 16:\n",
      "  Train Loss: 0.3854 | Train Acc: 0.9990 | Train F1: 0.9991\n",
      "  Val   Loss: 1.2241 | Val   Acc: 0.5247 | Val   F1: 0.5133\n",
      "Epoch 17:\n",
      "  Train Loss: 0.3828 | Train Acc: 1.0000 | Train F1: 1.0000\n",
      "  Val   Loss: 1.2062 | Val   Acc: 0.5202 | Val   F1: 0.4944\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 18:\n",
      "  Train Loss: 0.3761 | Train Acc: 1.0000 | Train F1: 1.0000\n",
      "  Val   Loss: 1.2175 | Val   Acc: 0.5157 | Val   F1: 0.5073\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 19:\n",
      "  Train Loss: 0.3710 | Train Acc: 1.0000 | Train F1: 1.0000\n",
      "  Val   Loss: 1.2240 | Val   Acc: 0.5561 | Val   F1: 0.5476\n",
      "Epoch 20:\n",
      "  Train Loss: 0.3718 | Train Acc: 1.0000 | Train F1: 1.0000\n",
      "  Val   Loss: 1.2254 | Val   Acc: 0.5650 | Val   F1: 0.5537\n",
      "Epoch 21:\n",
      "  Train Loss: 0.3683 | Train Acc: 1.0000 | Train F1: 1.0000\n",
      "  Val   Loss: 1.2101 | Val   Acc: 0.5516 | Val   F1: 0.5456\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 22:\n",
      "  Train Loss: 0.3677 | Train Acc: 1.0000 | Train F1: 1.0000\n",
      "  Val   Loss: 1.2087 | Val   Acc: 0.5157 | Val   F1: 0.5027\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 23:\n",
      "  Train Loss: 0.3652 | Train Acc: 1.0000 | Train F1: 1.0000\n",
      "  Val   Loss: 1.1959 | Val   Acc: 0.5426 | Val   F1: 0.5206\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 24:\n",
      "  Train Loss: 0.3620 | Train Acc: 1.0000 | Train F1: 1.0000\n",
      "  Val   Loss: 1.1986 | Val   Acc: 0.5516 | Val   F1: 0.5410\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 25:\n",
      "  Train Loss: 0.3603 | Train Acc: 1.0000 | Train F1: 1.0000\n",
      "  Val   Loss: 1.2045 | Val   Acc: 0.6099 | Val   F1: 0.6016\n",
      "Epoch 26:\n",
      "  Train Loss: 0.3598 | Train Acc: 1.0000 | Train F1: 1.0000\n",
      "  Val   Loss: 1.2069 | Val   Acc: 0.5695 | Val   F1: 0.5621\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 27:\n",
      "  Train Loss: 0.3586 | Train Acc: 1.0000 | Train F1: 1.0000\n",
      "  Val   Loss: 1.2160 | Val   Acc: 0.5874 | Val   F1: 0.5793\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 28:\n",
      "  Train Loss: 0.3567 | Train Acc: 1.0000 | Train F1: 1.0000\n",
      "  Val   Loss: 1.2039 | Val   Acc: 0.5919 | Val   F1: 0.5857\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 29:\n",
      "  Train Loss: 0.3555 | Train Acc: 1.0000 | Train F1: 1.0000\n",
      "  Val   Loss: 1.2031 | Val   Acc: 0.5605 | Val   F1: 0.5496\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 30:\n",
      "  Train Loss: 0.3553 | Train Acc: 1.0000 | Train F1: 1.0000\n",
      "  Val   Loss: 1.1948 | Val   Acc: 0.6009 | Val   F1: 0.5904\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 31:\n",
      "  Train Loss: 0.3547 | Train Acc: 1.0000 | Train F1: 1.0000\n",
      "  Val   Loss: 1.1974 | Val   Acc: 0.5695 | Val   F1: 0.5598\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch 32:\n",
      "  Train Loss: 0.3536 | Train Acc: 1.0000 | Train F1: 1.0000\n",
      "  Val   Loss: 1.1974 | Val   Acc: 0.5874 | Val   F1: 0.5813\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch 33:\n",
      "  Train Loss: 0.3534 | Train Acc: 1.0000 | Train F1: 1.0000\n",
      "  Val   Loss: 1.2073 | Val   Acc: 0.5740 | Val   F1: 0.5690\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch 34:\n",
      "  Train Loss: 0.3530 | Train Acc: 1.0000 | Train F1: 1.0000\n",
      "  Val   Loss: 1.2034 | Val   Acc: 0.5695 | Val   F1: 0.5622\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch 35:\n",
      "  Train Loss: 0.3528 | Train Acc: 1.0000 | Train F1: 1.0000\n",
      "  Val   Loss: 1.2055 | Val   Acc: 0.5785 | Val   F1: 0.5714\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping triggered!\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Check for CUDA availability and set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the path where the best model will be saved\n",
    "save_path = os.path.abspath(\"../checkpoints/best_transformer_model.pt\")\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_dim = 128  # Hidden dimension for the feedforward layer\n",
    "num_classes = 4  # Number of output classes\n",
    "learning_rate = 5e-5  # Learning rate for Adam optimizer\n",
    "weight_decay = 1e-2  # Weight decay (L2 regularization)\n",
    "max_epochs = 1000  # Maximum number of epochs\n",
    "patience = 10  # Patience for early stopping\n",
    "\n",
    "# Model initialization\n",
    "feature_dim = 833  # Input feature dimension (set according to your data)\n",
    "model = TransformerClassifier(\n",
    "    feature_dim=feature_dim,\n",
    "    hidden_dim=256,     # You can increase this value to make the hidden layers of transformer larger\n",
    "    num_classes=num_classes,\n",
    "    num_heads=4,        # Number of attention heads\n",
    "    num_layers=2,       # Number of transformer layers\n",
    "    dropout=0.1         # Dropout rate\n",
    ").to(device)\n",
    "\n",
    "# Optimizer: AdamW with weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Loss function: Custom smooth cross-entropy loss with label smoothing\n",
    "def smooth_cross_entropy(preds, targets, smoothing=0.1):\n",
    "    \"\"\"\n",
    "    Calculate smooth cross-entropy loss with label smoothing.\n",
    "\n",
    "    Args:\n",
    "        preds (tensor): Predicted logits (batch_size, num_classes)\n",
    "        targets (tensor): Ground truth labels (batch_size,)\n",
    "        smoothing (float): Smoothing factor for labels\n",
    "\n",
    "    Returns:\n",
    "        tensor: The mean smooth cross-entropy loss\n",
    "    \"\"\"\n",
    "    confidence = 1.0 - smoothing\n",
    "    logprobs = F.log_softmax(preds, dim=-1)  # Log of predicted probabilities\n",
    "    nll_loss = -logprobs.gather(dim=-1, index=targets.unsqueeze(1)).squeeze(1)  # Negative log-likelihood loss\n",
    "    smooth_loss = -logprobs.mean(dim=-1)  # Mean of the log probabilities (smooth part)\n",
    "    loss = confidence * nll_loss + smoothing * smooth_loss  # Weighted sum of NLL and smooth loss\n",
    "    return loss.mean()\n",
    "\n",
    "# Set the custom loss function\n",
    "loss_fn = smooth_cross_entropy\n",
    "\n",
    "# Initialize EarlyStopping object to monitor validation F1 score\n",
    "early_stopper = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    # Train for one epoch\n",
    "    train_loss, train_acc, train_f1 = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_loss, val_acc, val_f1 = evaluate_one_epoch(model, val_loader, loss_fn, device)\n",
    "\n",
    "    # Print training and validation metrics for the current epoch\n",
    "    print(f\"Epoch {epoch}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f}\")\n",
    "    print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f} | Val   F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Call EarlyStopping to check if training should be stopped\n",
    "    early_stopper(val_f1, model, save_path)\n",
    "\n",
    "    # Stop training early if the performance does not improve\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "# End of training\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb2804e3-5af0-4c37-9aaa-fd23b05fe2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final Test Results ===\n",
      "Test Loss: 1.2331\n",
      "Test Accuracy: 0.5223\n",
      "Test F1 Score: 0.5218\n"
     ]
    }
   ],
   "source": [
    "# Load the trained best model from the saved checkpoint\n",
    "model.load_state_dict(torch.load(save_path))  # Load the model's state_dict\n",
    "model.to(device)  # Move model to the appropriate device (CPU or GPU)\n",
    "model.eval()  # Set model to evaluation mode (disables dropout, etc.)\n",
    "\n",
    "# Define the testing function\n",
    "def test_model(model, test_loader, loss_fn, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Disable gradient computation during testing (saves memory and computations)\n",
    "    with torch.no_grad():\n",
    "        for x, mask, y in test_loader:\n",
    "            x = x.to(device)  # Move input data to the correct device\n",
    "            mask = mask.to(device)  # Move mask data to the correct device\n",
    "            y = y.to(device)  # Move target labels to the correct device\n",
    "\n",
    "            x = torch.nan_to_num(x, nan=0.0)  # Replace NaNs with 0s\n",
    "\n",
    "            # Pass the data through the model and get logits\n",
    "            logits, _ = model(x, mask)\n",
    "\n",
    "            # Clamp logits to a certain range for numerical stability\n",
    "            logits = torch.clamp(logits, min=-10, max=10)\n",
    "\n",
    "            # Calculate loss using the loss function\n",
    "            loss = loss_fn(logits, y)\n",
    "\n",
    "            running_loss += loss.item() * x.size(0)  # Accumulate loss\n",
    "\n",
    "            # Get predictions by taking the argmax (class with the highest probability)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "\n",
    "            # Append predictions and true labels to lists\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(y.cpu().tolist())\n",
    "\n",
    "    # Compute the average loss over the entire test set\n",
    "    test_loss = running_loss / len(test_loader.dataset)\n",
    "\n",
    "    # Calculate accuracy and F1 score\n",
    "    test_acc = accuracy_score(all_labels, all_preds)\n",
    "    test_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    return test_loss, test_acc, test_f1\n",
    "\n",
    "# Call the test function\n",
    "test_loss, test_acc, test_f1 = test_model(model, test_loader, loss_fn, device)\n",
    "\n",
    "# Print the final test results\n",
    "print(\"=== Final Test Results ===\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b7984a-7c2b-4904-bcde-492b327af2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_214",
   "language": "python",
   "name": "env_214"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
