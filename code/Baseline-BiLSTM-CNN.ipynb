{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "851af380-15ef-438e-ad51-04472348f1e6",
   "metadata": {},
   "source": [
    "### Step 1. prepare Dataset + DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78c4fa93-f636-4f5a-ac6b-92effa9dda06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Twitter15Dataset(Dataset):\n",
    "    def __init__(self, graph_data_list):\n",
    "        \"\"\"\n",
    "        Custom dataset for Twitter15 graph data.\n",
    "\n",
    "        Args:\n",
    "            graph_data_list (list): List of graph samples. \n",
    "                                    Each graph is a dictionary with keys: \n",
    "                                    - 'x': node feature matrix (Tensor of shape [seq_len, feature_dim])\n",
    "                                    - 'edge_index': edge list (not used here)\n",
    "                                    - 'y': label (int in [0, 3])\n",
    "        \"\"\"\n",
    "        self.graphs = graph_data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of samples in the dataset\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the features and label for a given index\n",
    "        graph = self.graphs[idx]\n",
    "        x = graph['x']  # Feature tensor: (sequence_length, feature_dim)\n",
    "        y = graph['y']  # Label: integer in range [0, 3]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function to handle batches of variable-length sequences.\n",
    "    Pads each sequence in the batch to the length of the longest sequence.\n",
    "\n",
    "    Args:\n",
    "        batch (list of tuples): Each tuple is (x, y), where\n",
    "                                - x: Tensor of shape (seq_len, feature_dim)\n",
    "                                - y: label (int)\n",
    "\n",
    "    Returns:\n",
    "        padded_xs (Tensor): Padded input features (batch_size, max_seq_len, feature_dim)\n",
    "        masks (Tensor): Boolean masks indicating valid positions (batch_size, max_seq_len)\n",
    "        ys (Tensor): Tensor of labels (batch_size,)\n",
    "    \"\"\"\n",
    "    xs, ys = zip(*batch)\n",
    "\n",
    "    # Determine the maximum sequence length in this batch\n",
    "    max_len = max(x.shape[0] for x in xs)\n",
    "    feature_dim = xs[0].shape[1]\n",
    "\n",
    "    padded_xs = []\n",
    "    masks = []\n",
    "\n",
    "    for x in xs:\n",
    "        seq_len = x.shape[0]\n",
    "        pad_len = max_len - seq_len\n",
    "\n",
    "        if pad_len > 0:\n",
    "            # Pad with zeros at the end of the sequence\n",
    "            pad = torch.zeros((pad_len, feature_dim), dtype=x.dtype)\n",
    "            x_padded = torch.cat([x, pad], dim=0)\n",
    "        else:\n",
    "            x_padded = x\n",
    "\n",
    "        # Create a mask where True indicates valid (non-padded) positions\n",
    "        mask = torch.cat([torch.ones(seq_len), torch.zeros(pad_len)]).bool()\n",
    "\n",
    "        padded_xs.append(x_padded)\n",
    "        masks.append(mask)\n",
    "\n",
    "    # Stack all sequences and masks into tensors\n",
    "    padded_xs = torch.stack(padded_xs)    # Shape: (batch_size, max_len, feature_dim)\n",
    "    masks = torch.stack(masks)            # Shape: (batch_size, max_len)\n",
    "    ys = torch.tensor(ys)                 # Shape: (batch_size,)\n",
    "\n",
    "    return padded_xs, masks, ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d2a3f99-0e53-4ce0-b328-e5a32368170a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1043, Val: 223, Test: 224\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# Load your cleaned graph data\n",
    "graph_data_list = torch.load(\"../processed/twitter15_graph_data_clean.pt\", weights_only=False)\n",
    "\n",
    "# Split into Train / Validation / Test sets in a 7:1.5:1.5 ratio\n",
    "train_graphs, temp_graphs = train_test_split(graph_data_list, test_size=0.3, random_state=42)\n",
    "val_graphs, test_graphs = train_test_split(temp_graphs, test_size=0.5, random_state=42)\n",
    "\n",
    "# Output the number of samples in each split\n",
    "print(f\"Train: {len(train_graphs)}, Val: {len(val_graphs)}, Test: {len(test_graphs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1a066f7-510d-4732-bb5e-775da05f5cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created successfully!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Build Dataset objects for training, validation, and test sets\n",
    "train_dataset = Twitter15Dataset(train_graphs)\n",
    "val_dataset = Twitter15Dataset(val_graphs)\n",
    "test_dataset = Twitter15Dataset(test_graphs)\n",
    "\n",
    "# Build DataLoaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(\"DataLoaders created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820b6a26-4e2c-4b7e-9e8d-fe2966327260",
   "metadata": {},
   "source": [
    "### Step 2: BiLSTM-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b613c42f-d68d-417c-ac2a-95ae85b3675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class BiLSTM_CNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_classes, kernel_size=3):\n",
    "        super(BiLSTM_CNN, self).__init__()\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=833,               # Input feature dimension\n",
    "            hidden_size=hidden_dim,       # Hidden size of the LSTM\n",
    "            bidirectional=True,           # Use bidirectional LSTM\n",
    "            batch_first=True,             # Input and output tensors are provided as (batch, seq, feature)\n",
    "            dropout=0.2                   # Added dropout to prevent overfitting\n",
    "        )\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=hidden_dim * 2,   # BiLSTM outputs 2 * hidden_dim channels\n",
    "            out_channels=hidden_dim,      # Output channels after 1D convolution\n",
    "            kernel_size=kernel_size,\n",
    "            padding=kernel_size // 2      # Added padding to preserve sequence length\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)  # Final classification layer\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Pack the padded sequence for LSTM\n",
    "        packed_input = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        lstm_out, _ = self.bilstm(packed_input)\n",
    "        \n",
    "        # Unpack sequence\n",
    "        lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True)\n",
    "\n",
    "        # Transpose for Conv1d: (batch_size, channels, seq_len)\n",
    "        lstm_out = lstm_out.permute(0, 2, 1)\n",
    "        cnn_out = self.conv1d(lstm_out)             # Apply 1D convolution\n",
    "        cnn_out = cnn_out.max(dim=2)[0]             # Max pooling over time (seq_len dimension)\n",
    "\n",
    "        output = self.fc(cnn_out)                   # Fully connected layer for classification\n",
    "\n",
    "        return output, lstm_out                     # Return both prediction and LSTM features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6690684a-800d-4030-b1eb-d77aef088022",
   "metadata": {},
   "source": [
    "### Step 3. Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56735c89-d1ba-4314-8468-98f6df234351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, loss_fn, device):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for x, mask, y in train_loader:\n",
    "        # Move tensors to the specified device\n",
    "        x = x.to(device)\n",
    "        mask = mask.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Replace NaNs with zeros to prevent propagation issues\n",
    "        x = torch.nan_to_num(x, nan=0.0)\n",
    "\n",
    "        # Compute the actual sequence lengths from the mask\n",
    "        lengths = mask.sum(dim=1).cpu()\n",
    "\n",
    "        # Forward pass\n",
    "        logits, _ = model(x, lengths)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "\n",
    "        # Get predicted class indices\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        all_preds.extend(preds.detach().cpu().tolist())\n",
    "        all_labels.extend(y.cpu().tolist())\n",
    "\n",
    "    # Compute average metrics for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    epoch_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    return epoch_loss, epoch_acc, epoch_f1\n",
    "\n",
    "\n",
    "def evaluate_one_epoch(model, val_loader, loss_fn, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for faster inference\n",
    "        for x, mask, y in val_loader:\n",
    "            x = x.to(device)\n",
    "            mask = mask.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            x = torch.nan_to_num(x, nan=0.0)  # Replace NaNs with zeros\n",
    "\n",
    "            lengths = mask.sum(dim=1).cpu()   # Compute actual lengths\n",
    "            logits, _ = model(x, lengths)     # Forward pass\n",
    "\n",
    "            # Clamp logits to avoid numerical overflow\n",
    "            logits = torch.clamp(logits, min=-10, max=10)\n",
    "\n",
    "            loss = loss_fn(logits, y)\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(y.cpu().tolist())\n",
    "\n",
    "    # Compute average validation metrics\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    epoch_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    return epoch_loss, epoch_acc, epoch_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86a168a-6e41-461a-aa73-4a7014c0ec9c",
   "metadata": {},
   "source": [
    "### Step 4: Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ae78cff-6175-45b9-8539-2b5fb3d5ec35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/xzhan/.conda/envs/env_214/lib/python3.13/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "  Train Loss: 1.3854 | Train Acc: 0.2733 | Train F1: 0.2127\n",
      "  Val   Loss: 1.3844 | Val   Acc: 0.2825 | Val   F1: 0.2697\n",
      "Epoch 2:\n",
      "  Train Loss: 1.3643 | Train Acc: 0.3988 | Train F1: 0.3627\n",
      "  Val   Loss: 1.3698 | Val   Acc: 0.3767 | Val   F1: 0.3689\n",
      "Epoch 3:\n",
      "  Train Loss: 1.3414 | Train Acc: 0.4593 | Train F1: 0.4492\n",
      "  Val   Loss: 1.3595 | Val   Acc: 0.3587 | Val   F1: 0.3467\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 4:\n",
      "  Train Loss: 1.3019 | Train Acc: 0.5283 | Train F1: 0.5242\n",
      "  Val   Loss: 1.3363 | Val   Acc: 0.3991 | Val   F1: 0.3839\n",
      "Epoch 5:\n",
      "  Train Loss: 1.2459 | Train Acc: 0.5551 | Train F1: 0.5455\n",
      "  Val   Loss: 1.3026 | Val   Acc: 0.3901 | Val   F1: 0.3856\n",
      "Epoch 6:\n",
      "  Train Loss: 1.1772 | Train Acc: 0.5858 | Train F1: 0.5736\n",
      "  Val   Loss: 1.2704 | Val   Acc: 0.4529 | Val   F1: 0.4527\n",
      "Epoch 7:\n",
      "  Train Loss: 1.1129 | Train Acc: 0.6194 | Train F1: 0.6154\n",
      "  Val   Loss: 1.2622 | Val   Acc: 0.4484 | Val   F1: 0.4463\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 8:\n",
      "  Train Loss: 1.0526 | Train Acc: 0.6433 | Train F1: 0.6369\n",
      "  Val   Loss: 1.2537 | Val   Acc: 0.4529 | Val   F1: 0.4547\n",
      "Epoch 9:\n",
      "  Train Loss: 0.9820 | Train Acc: 0.6989 | Train F1: 0.6940\n",
      "  Val   Loss: 1.2453 | Val   Acc: 0.4709 | Val   F1: 0.4715\n",
      "Epoch 10:\n",
      "  Train Loss: 0.9255 | Train Acc: 0.7325 | Train F1: 0.7303\n",
      "  Val   Loss: 1.2383 | Val   Acc: 0.4798 | Val   F1: 0.4812\n",
      "Epoch 11:\n",
      "  Train Loss: 0.8669 | Train Acc: 0.7661 | Train F1: 0.7635\n",
      "  Val   Loss: 1.2278 | Val   Acc: 0.5202 | Val   F1: 0.5235\n",
      "Epoch 12:\n",
      "  Train Loss: 0.8136 | Train Acc: 0.7939 | Train F1: 0.7913\n",
      "  Val   Loss: 1.2543 | Val   Acc: 0.5112 | Val   F1: 0.5158\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 13:\n",
      "  Train Loss: 0.7612 | Train Acc: 0.8313 | Train F1: 0.8295\n",
      "  Val   Loss: 1.2389 | Val   Acc: 0.5157 | Val   F1: 0.5198\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 14:\n",
      "  Train Loss: 0.7128 | Train Acc: 0.8370 | Train F1: 0.8352\n",
      "  Val   Loss: 1.2763 | Val   Acc: 0.4888 | Val   F1: 0.4892\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 15:\n",
      "  Train Loss: 0.6648 | Train Acc: 0.8686 | Train F1: 0.8675\n",
      "  Val   Loss: 1.2855 | Val   Acc: 0.5112 | Val   F1: 0.5115\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 16:\n",
      "  Train Loss: 0.6199 | Train Acc: 0.8965 | Train F1: 0.8956\n",
      "  Val   Loss: 1.3148 | Val   Acc: 0.5022 | Val   F1: 0.5029\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 17:\n",
      "  Train Loss: 0.5859 | Train Acc: 0.9089 | Train F1: 0.9079\n",
      "  Val   Loss: 1.3340 | Val   Acc: 0.5291 | Val   F1: 0.5306\n",
      "Epoch 18:\n",
      "  Train Loss: 0.5466 | Train Acc: 0.9300 | Train F1: 0.9296\n",
      "  Val   Loss: 1.3364 | Val   Acc: 0.5247 | Val   F1: 0.5228\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 19:\n",
      "  Train Loss: 0.5169 | Train Acc: 0.9463 | Train F1: 0.9457\n",
      "  Val   Loss: 1.3442 | Val   Acc: 0.5336 | Val   F1: 0.5330\n",
      "Epoch 20:\n",
      "  Train Loss: 0.4894 | Train Acc: 0.9607 | Train F1: 0.9602\n",
      "  Val   Loss: 1.3681 | Val   Acc: 0.5202 | Val   F1: 0.5196\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 21:\n",
      "  Train Loss: 0.4703 | Train Acc: 0.9760 | Train F1: 0.9757\n",
      "  Val   Loss: 1.4327 | Val   Acc: 0.4933 | Val   F1: 0.4924\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 22:\n",
      "  Train Loss: 0.4467 | Train Acc: 0.9856 | Train F1: 0.9854\n",
      "  Val   Loss: 1.4446 | Val   Acc: 0.5112 | Val   F1: 0.5072\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 23:\n",
      "  Train Loss: 0.4312 | Train Acc: 0.9885 | Train F1: 0.9884\n",
      "  Val   Loss: 1.4092 | Val   Acc: 0.5471 | Val   F1: 0.5445\n",
      "Epoch 24:\n",
      "  Train Loss: 0.4160 | Train Acc: 0.9952 | Train F1: 0.9951\n",
      "  Val   Loss: 1.4711 | Val   Acc: 0.5022 | Val   F1: 0.5008\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 25:\n",
      "  Train Loss: 0.4092 | Train Acc: 0.9942 | Train F1: 0.9942\n",
      "  Val   Loss: 1.4308 | Val   Acc: 0.5336 | Val   F1: 0.5335\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 26:\n",
      "  Train Loss: 0.3996 | Train Acc: 0.9971 | Train F1: 0.9971\n",
      "  Val   Loss: 1.4275 | Val   Acc: 0.5022 | Val   F1: 0.5010\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 27:\n",
      "  Train Loss: 0.3925 | Train Acc: 0.9971 | Train F1: 0.9971\n",
      "  Val   Loss: 1.4267 | Val   Acc: 0.5202 | Val   F1: 0.5181\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 28:\n",
      "  Train Loss: 0.3841 | Train Acc: 1.0000 | Train F1: 1.0000\n",
      "  Val   Loss: 1.4382 | Val   Acc: 0.4978 | Val   F1: 0.4952\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 29:\n",
      "  Train Loss: 0.3784 | Train Acc: 1.0000 | Train F1: 1.0000\n",
      "  Val   Loss: 1.4181 | Val   Acc: 0.5067 | Val   F1: 0.5072\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch 30:\n",
      "  Train Loss: 0.3770 | Train Acc: 0.9981 | Train F1: 0.9981\n",
      "  Val   Loss: 1.4454 | Val   Acc: 0.5067 | Val   F1: 0.5046\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch 31:\n",
      "  Train Loss: 0.3720 | Train Acc: 1.0000 | Train F1: 1.0000\n",
      "  Val   Loss: 1.4016 | Val   Acc: 0.5426 | Val   F1: 0.5395\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch 32:\n",
      "  Train Loss: 0.3685 | Train Acc: 1.0000 | Train F1: 1.0000\n",
      "  Val   Loss: 1.4042 | Val   Acc: 0.5112 | Val   F1: 0.5117\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch 33:\n",
      "  Train Loss: 0.3672 | Train Acc: 0.9990 | Train F1: 0.9991\n",
      "  Val   Loss: 1.3863 | Val   Acc: 0.5202 | Val   F1: 0.5194\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping triggered!\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Device configuration: use GPU if available, else fallback to CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "hidden_dim = 128\n",
    "num_classes = 4\n",
    "learning_rate = 5e-5\n",
    "weight_decay = 1e-2\n",
    "max_epochs = 1000         # Maximum number of epochs\n",
    "patience = 10             # Patience for early stopping\n",
    "\n",
    "# Path to save the best model\n",
    "save_path = os.path.abspath(\"../checkpoints/best_model.pt\")\n",
    "\n",
    "# Initialize the model and move it to the selected device\n",
    "model = BiLSTM_CNN(hidden_dim, num_classes).to(device)\n",
    "\n",
    "# Define optimizer (AdamW helps with weight decay regularization)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Define loss function with label smoothing to improve generalization\n",
    "def smooth_cross_entropy(preds, targets, smoothing=0.1):\n",
    "    confidence = 1.0 - smoothing\n",
    "    logprobs = F.log_softmax(preds, dim=-1)\n",
    "    nll_loss = -logprobs.gather(dim=-1, index=targets.unsqueeze(1)).squeeze(1)\n",
    "    smooth_loss = -logprobs.mean(dim=-1)\n",
    "    loss = confidence * nll_loss + smoothing * smooth_loss\n",
    "    return loss.mean()\n",
    "\n",
    "loss_fn = smooth_cross_entropy\n",
    "\n",
    "# EarlyStopping utility to stop training when validation performance stops improving\n",
    "early_stopper = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    # Train for one epoch\n",
    "    train_loss, train_acc, train_f1 = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_loss, val_acc, val_f1 = evaluate_one_epoch(model, val_loader, loss_fn, device)\n",
    "\n",
    "    # Print performance metrics\n",
    "    print(f\"Epoch {epoch}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f}\")\n",
    "    print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f} | Val   F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    early_stopper(val_f1, model, save_path)\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb2804e3-5af0-4c37-9aaa-fd23b05fe2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final Test Results ===\n",
      "Test Loss: 1.3627\n",
      "Test Accuracy: 0.5045\n",
      "Test F1 Score: 0.5043\n"
     ]
    }
   ],
   "source": [
    "# Load the best trained model (previously saved)\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode (important for dropout/batchnorm layers)\n",
    "\n",
    "# Test function\n",
    "def test_model(model, test_loader, loss_fn, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    all_preds = []  # To store predictions\n",
    "    all_labels = []  # To store true labels\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation, as it's not needed during evaluation\n",
    "        for x, mask, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            mask = mask.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Replace any NaNs in the input with 0.0\n",
    "            x = torch.nan_to_num(x, nan=0.0)\n",
    "\n",
    "            # Calculate sequence lengths from the mask\n",
    "            lengths = mask.sum(dim=1).cpu()\n",
    "\n",
    "            # Forward pass through the model\n",
    "            logits, _ = model(x, lengths)\n",
    "            logits = torch.clamp(logits, min=-10, max=10)  # Clip logits to avoid extreme values\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(logits, y)\n",
    "            running_loss += loss.item() * x.size(0)  # Accumulate the loss\n",
    "\n",
    "            # Get the predictions (take the index with the highest logit for each sample)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            all_preds.extend(preds.cpu().tolist())  # Append predictions to the list\n",
    "            all_labels.extend(y.cpu().tolist())  # Append true labels to the list\n",
    "\n",
    "    # Calculate average test loss\n",
    "    test_loss = running_loss / len(test_loader.dataset)\n",
    "\n",
    "    # Calculate accuracy and F1 score for evaluation\n",
    "    test_acc = accuracy_score(all_labels, all_preds)\n",
    "    test_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    return test_loss, test_acc, test_f1\n",
    "\n",
    "# Call the test function to evaluate the model\n",
    "test_loss, test_acc, test_f1 = test_model(model, test_loader, loss_fn, device)\n",
    "\n",
    "# Print final results\n",
    "print(\"=== Final Test Results ===\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b7984a-7c2b-4904-bcde-492b327af2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_214",
   "language": "python",
   "name": "env_214"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
